{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple q-learning agent with experience replay\n",
    "\n",
    "We re-write q-learning algorithm using _agentnet_ - a helper for lasagne that implements some RL techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting virtual X frame buffer: Xvfb../xvfb: line 8: start-stop-daemon: command not found\n",
      ".\n",
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup\n",
    "* Here we simply load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:19:06,702] Making new env: Acrobot-v1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "make_env = lambda: gym.make(\"Acrobot-v1\")\n",
    "\n",
    "env=make_env()\n",
    "env.reset()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAFkCAYAAADbgnvLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110ed3d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(None, None,) + state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import elu\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "#<define a network layer here. We recommend 2-3 layers of 100~300 neurons for\n",
    "observation_layer = InputLayer((None,)+state_shape)\n",
    "d_layer = DenseLayer(observation_layer, 256, nonlinearity=elu)\n",
    "d_layer2 = DenseLayer(d_layer, 64, nonlinearity=elu)\n",
    "d_layer3 = DenseLayer(d_layer2, 256, nonlinearity=elu)\n",
    "#nn = LSTMLayer(observation_layer, 128, nonlinearity=elu)\n",
    "\n",
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(d_layer3, num_units=n_actions, nonlinearity=None, name=\"q-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking actions is done by yet another layer, that implements $ \\epsilon$ -greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)\n",
    "\n",
    "#set starting epsilon\n",
    "action_layer.epsilon.set_value(np.float32(0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "We define an agent entirely composed of a lasagne network:\n",
    "* Observations as InputLayer(s)\n",
    "* Actions as intermediate Layer(s)\n",
    "* `policy_estimators` is \"whatever else you want to keep track of\"\n",
    "\n",
    "Each parameter can be either one layer or a list of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              action_layers=action_layer,\n",
    "              policy_estimators=qvalues_layer,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, W, b, q-values.W, q-values.b]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:22:52,750] Making new env: Acrobot-v1\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "pool = EnvPool(agent,make_env,n_games=1,max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('actions:', array([[0, 2, 2, 0, 0]]))\n",
      "('rewards:', array([[-1., -1., -1., -1.,  0.]]))\n",
      "CPU times: user 5.45 ms, sys: 2.34 ms, total: 7.79 ms\n",
      "Wall time: 8.79 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "obs_log,action_log,reward_log,_,_,_  = pool.interact(5)\n",
    "\n",
    "\n",
    "print('actions:',action_log)\n",
    "print('rewards:',reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleg/Anaconda/envs/py2/lib/python2.7/site-packages/agentnet/utils/logging.py:14: UserWarning: [Verbose>=1] Warning! Appending sessions to empty or broken pool. Old pool sessions, if any, are disposed.\n",
      "  default_warn(\"[Verbose>=%s] %s\"%(verbosity_level,message),**kwargs)\n"
     ]
    }
   ],
   "source": [
    "#we'll train on rollouts of 10 steps (required by n-step algorithms and rnns later)\n",
    "SEQ_LENGTH=10\n",
    "\n",
    "#load first sessions (this function calls interact and stores sessions in the pool)\n",
    "\n",
    "for _ in range(100):\n",
    "    pool.update(SEQ_LENGTH, append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pool.experience_replay[0].get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q-learning\n",
    "\n",
    "We shall now define a function that replays recent game sessions and updates network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100)\n",
    "qvalues_seq = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2, like you implemented before in lasagne.\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      gamma_or_gammas=0.99,\n",
    "                                                      n_steps=1,)\n",
    "\n",
    "#compute mean loss over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get weight updates\n",
    "updates = lasagne.updates.adam(loss,weights,learning_rate=1e-4)\n",
    "\n",
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run\n",
    "\n",
    "Play full session with an untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:23:40,032] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:23:40,041] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:23:40,043] Creating monitor directory ./records\n",
      "[2017-05-05 17:23:40,046] Starting new video recorder writing to /Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records/openaigym.video.0.66732.video000000.mp4\n",
      "[2017-05-05 17:23:45,641] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n"
     ]
    }
   ],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./records/openaigym.video.0.66732.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./records/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_counter = 1 #starting epoch\n",
    "rewards = {} #full game rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 99/5000 [00:04<03:52, 21.08it/s][2017-05-05 17:24:51,511] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:24:51,519] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:24:51,520] Clearing 4 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "Episode finished after 500 timesteps with reward=-500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:24:52,097] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      "  2%|▏         | 102/5000 [00:05<08:49,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "iter=100\tepsilon=0.910\n",
      "Current score(mean over 3) = -500.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 198/5000 [00:10<03:49, 20.96it/s][2017-05-05 17:24:57,311] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:24:57,318] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:24:57,319] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "Episode finished after 500 timesteps with reward=-500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:24:58,012] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      "  4%|▍         | 201/5000 [00:11<09:34,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "iter=200\tepsilon=0.828\n",
      "Current score(mean over 3) = -500.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 297/5000 [00:16<03:59, 19.64it/s][2017-05-05 17:25:03,362] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:25:03,371] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:25:03,373] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "Episode finished after 500 timesteps with reward=-500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:25:03,859] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      "  6%|▌         | 302/5000 [00:17<06:37, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 417 timesteps with reward=-416.0\n",
      "iter=300\tepsilon=0.754\n",
      "Current score(mean over 3) = -472.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 399/5000 [00:23<04:32, 16.89it/s][2017-05-05 17:25:10,387] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:25:10,395] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:25:10,396] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:25:10,672] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 244 timesteps with reward=-243.0\n",
      "Episode finished after 241 timesteps with reward=-240.0\n",
      "Episode finished after 231 timesteps with reward=-230.0\n",
      "iter=400\tepsilon=0.687\n",
      "Current score(mean over 3) = -237.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 499/5000 [00:29<04:25, 16.98it/s][2017-05-05 17:25:16,419] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:25:16,428] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:25:16,429] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 197 timesteps with reward=-196.0\n",
      "Episode finished after 372 timesteps with reward=-371.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:25:16,747] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 10%|█         | 503/5000 [00:30<06:53, 10.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 224 timesteps with reward=-223.0\n",
      "iter=500\tepsilon=0.626\n",
      "Current score(mean over 3) = -263.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 599/5000 [00:37<04:34, 16.03it/s][2017-05-05 17:25:23,880] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:25:23,889] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:25:23,890] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:25:24,150] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 260 timesteps with reward=-259.0\n",
      "Episode finished after 194 timesteps with reward=-193.0\n",
      "Episode finished after 196 timesteps with reward=-195.0\n",
      "iter=600\tepsilon=0.571\n",
      "Current score(mean over 3) = -215.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 699/5000 [00:44<05:28, 13.08it/s][2017-05-05 17:25:30,830] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:25:30,839] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:25:30,840] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 140 timesteps with reward=-139.0\n",
      "Episode finished after 208 timesteps with reward=-207.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:25:31,295] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 14%|█▍        | 701/5000 [00:44<10:56,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 314 timesteps with reward=-313.0\n",
      "iter=700\tepsilon=0.522\n",
      "Current score(mean over 3) = -219.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 799/5000 [00:51<04:48, 14.57it/s][2017-05-05 17:25:38,082] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:25:38,092] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:25:38,094] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:25:38,374] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 154 timesteps with reward=-153.0\n",
      "Episode finished after 248 timesteps with reward=-247.0\n",
      "Episode finished after 199 timesteps with reward=-198.0\n",
      "iter=800\tepsilon=0.477\n",
      "Current score(mean over 3) = -199.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 899/5000 [00:57<04:04, 16.75it/s][2017-05-05 17:25:44,500] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:25:44,508] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:25:44,509] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:25:44,705] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 18%|█▊        | 901/5000 [00:58<06:09, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 157 timesteps with reward=-156.0\n",
      "Episode finished after 159 timesteps with reward=-158.0\n",
      "Episode finished after 221 timesteps with reward=-220.0\n",
      "iter=900\tepsilon=0.436\n",
      "Current score(mean over 3) = -178.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 998/5000 [01:07<08:20,  8.00it/s][2017-05-05 17:25:54,557] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:25:54,565] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:25:54,566] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:25:54,759] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 20%|██        | 1000/5000 [01:08<09:22,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 165 timesteps with reward=-164.0\n",
      "Episode finished after 114 timesteps with reward=-113.0\n",
      "Episode finished after 174 timesteps with reward=-173.0\n",
      "iter=1000\tepsilon=0.399\n",
      "Current score(mean over 3) = -150.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1098/5000 [01:15<04:43, 13.74it/s][2017-05-05 17:26:02,198] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:26:02,206] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:26:02,207] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:26:02,351] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 22%|██▏       | 1100/5000 [01:15<06:14, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 122 timesteps with reward=-121.0\n",
      "Episode finished after 139 timesteps with reward=-138.0\n",
      "Episode finished after 96 timesteps with reward=-95.0\n",
      "iter=1100\tepsilon=0.366\n",
      "Current score(mean over 3) = -118.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1199/5000 [01:23<06:28,  9.77it/s][2017-05-05 17:26:10,252] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:26:10,259] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:26:10,260] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:26:10,433] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 24%|██▍       | 1201/5000 [01:23<07:48,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 167 timesteps with reward=-166.0\n",
      "Episode finished after 117 timesteps with reward=-116.0\n",
      "Episode finished after 155 timesteps with reward=-154.0\n",
      "iter=1200\tepsilon=0.336\n",
      "Current score(mean over 3) = -145.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1299/5000 [01:32<08:05,  7.62it/s][2017-05-05 17:26:19,528] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:26:19,537] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:26:19,539] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:26:19,723] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 26%|██▌       | 1300/5000 [01:33<11:44,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 160 timesteps with reward=-159.0\n",
      "Episode finished after 103 timesteps with reward=-102.0\n",
      "Episode finished after 120 timesteps with reward=-119.0\n",
      "iter=1300\tepsilon=0.309\n",
      "Current score(mean over 3) = -126.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1399/5000 [01:40<04:23, 13.66it/s][2017-05-05 17:26:27,498] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:26:27,505] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:26:27,506] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:26:27,628] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 28%|██▊       | 1401/5000 [01:41<05:33, 10.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 134 timesteps with reward=-133.0\n",
      "Episode finished after 119 timesteps with reward=-118.0\n",
      "Episode finished after 79 timesteps with reward=-78.0\n",
      "iter=1400\tepsilon=0.284\n",
      "Current score(mean over 3) = -109.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1499/5000 [01:48<04:27, 13.10it/s][2017-05-05 17:26:35,139] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:26:35,148] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:26:35,149] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:26:35,401] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 252 timesteps with reward=-251.0\n",
      "Episode finished after 190 timesteps with reward=-189.0\n",
      "Episode finished after 280 timesteps with reward=-279.0\n",
      "iter=1500\tepsilon=0.262\n",
      "Current score(mean over 3) = -239.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1599/5000 [01:58<07:30,  7.55it/s][2017-05-05 17:26:45,587] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:26:45,597] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:26:45,599] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 236 timesteps with reward=-235.0\n",
      "Episode finished after 197 timesteps with reward=-196.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:26:46,030] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 32%|███▏      | 1601/5000 [01:59<12:28,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 297 timesteps with reward=-296.0\n",
      "iter=1600\tepsilon=0.242\n",
      "Current score(mean over 3) = -242.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1699/5000 [02:10<06:45,  8.14it/s][2017-05-05 17:26:57,357] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:26:57,367] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:26:57,370] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 162 timesteps with reward=-161.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:26:57,805] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 34%|███▍      | 1700/5000 [02:11<14:54,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 265 timesteps with reward=-264.0\n",
      "Episode finished after 167 timesteps with reward=-166.0\n",
      "iter=1700\tepsilon=0.224\n",
      "Current score(mean over 3) = -197.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1799/5000 [02:23<06:44,  7.90it/s][2017-05-05 17:27:10,300] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:27:10,309] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:27:10,311] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 366 timesteps with reward=-365.0\n",
      "Episode finished after 314 timesteps with reward=-313.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:27:10,800] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 36%|███▌      | 1801/5000 [02:24<12:04,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 398 timesteps with reward=-397.0\n",
      "iter=1800\tepsilon=0.207\n",
      "Current score(mean over 3) = -358.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1899/5000 [02:34<06:00,  8.59it/s][2017-05-05 17:27:21,175] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:27:21,183] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:27:21,185] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:27:21,441] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 38%|███▊      | 1900/5000 [02:34<10:03,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 247 timesteps with reward=-246.0\n",
      "Episode finished after 144 timesteps with reward=-143.0\n",
      "Episode finished after 212 timesteps with reward=-211.0\n",
      "iter=1900\tepsilon=0.192\n",
      "Current score(mean over 3) = -200.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 1999/5000 [02:46<07:20,  6.81it/s][2017-05-05 17:27:33,114] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:27:33,122] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:27:33,123] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:27:33,250] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 40%|████      | 2000/5000 [02:46<08:37,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 110 timesteps with reward=-109.0\n",
      "Episode finished after 110 timesteps with reward=-109.0\n",
      "Episode finished after 138 timesteps with reward=-137.0\n",
      "iter=2000\tepsilon=0.179\n",
      "Current score(mean over 3) = -118.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2098/5000 [02:57<04:41, 10.32it/s][2017-05-05 17:27:43,900] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:27:43,908] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:27:43,910] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:27:44,057] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 42%|████▏     | 2100/5000 [02:57<05:41,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 89 timesteps with reward=-88.0\n",
      "Episode finished after 170 timesteps with reward=-169.0\n",
      "Episode finished after 119 timesteps with reward=-118.0\n",
      "iter=2100\tepsilon=0.166\n",
      "Current score(mean over 3) = -125.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2198/5000 [03:07<04:16, 10.93it/s][2017-05-05 17:27:53,894] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:27:53,901] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:27:53,903] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:27:54,185] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 44%|████▍     | 2200/5000 [03:07<06:20,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 366 timesteps with reward=-365.0\n",
      "Episode finished after 232 timesteps with reward=-231.0\n",
      "Episode finished after 127 timesteps with reward=-126.0\n",
      "iter=2200\tepsilon=0.155\n",
      "Current score(mean over 3) = -240.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2298/5000 [03:16<03:57, 11.40it/s][2017-05-05 17:28:03,606] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:28:03,614] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:28:03,615] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps with reward=-199.0\n",
      "Episode finished after 215 timesteps with reward=-214.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:28:03,941] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 46%|████▌     | 2300/5000 [03:17<06:11,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 261 timesteps with reward=-260.0\n",
      "iter=2300\tepsilon=0.145\n",
      "Current score(mean over 3) = -224.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 2398/5000 [03:26<03:35, 12.05it/s][2017-05-05 17:28:13,621] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:28:13,629] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:28:13,630] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 292 timesteps with reward=-291.0\n",
      "Episode finished after 390 timesteps with reward=-389.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:28:14,091] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 48%|████▊     | 2402/5000 [03:27<05:53,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 495 timesteps with reward=-494.0\n",
      "iter=2400\tepsilon=0.136\n",
      "Current score(mean over 3) = -391.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 2498/5000 [03:35<03:24, 12.21it/s][2017-05-05 17:28:22,460] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:28:22,468] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:28:22,469] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 285 timesteps with reward=-284.0\n",
      "Episode finished after 388 timesteps with reward=-387.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:28:22,877] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 50%|█████     | 2502/5000 [03:36<05:24,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 496 timesteps with reward=-495.0\n",
      "iter=2500\tepsilon=0.128\n",
      "Current score(mean over 3) = -388.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2599/5000 [03:45<03:19, 12.02it/s][2017-05-05 17:28:31,831] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:28:31,839] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:28:31,840] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:28:32,066] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 52%|█████▏    | 2601/5000 [03:45<04:46,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 320 timesteps with reward=-319.0\n",
      "Episode finished after 164 timesteps with reward=-163.0\n",
      "Episode finished after 134 timesteps with reward=-133.0\n",
      "iter=2600\tepsilon=0.121\n",
      "Current score(mean over 3) = -205.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 2699/5000 [03:53<03:10, 12.10it/s][2017-05-05 17:28:40,355] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:28:40,363] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:28:40,364] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:28:40,466] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 54%|█████▍    | 2701/5000 [03:53<03:49, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 88 timesteps with reward=-87.0\n",
      "Episode finished after 103 timesteps with reward=-102.0\n",
      "Episode finished after 77 timesteps with reward=-76.0\n",
      "iter=2700\tepsilon=0.114\n",
      "Current score(mean over 3) = -88.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 2799/5000 [04:02<03:02, 12.08it/s][2017-05-05 17:28:48,770] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:28:48,778] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:28:48,779] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:28:48,921] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 56%|█████▌    | 2801/5000 [04:02<03:54,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 103 timesteps with reward=-102.0\n",
      "Episode finished after 146 timesteps with reward=-145.0\n",
      "Episode finished after 135 timesteps with reward=-134.0\n",
      "iter=2800\tepsilon=0.108\n",
      "Current score(mean over 3) = -127.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 2898/5000 [04:12<03:38,  9.61it/s][2017-05-05 17:28:59,034] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:28:59,041] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:28:59,043] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:28:59,337] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 58%|█████▊    | 2900/5000 [04:12<05:02,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 401 timesteps with reward=-400.0\n",
      "Episode finished after 159 timesteps with reward=-158.0\n",
      "Episode finished after 135 timesteps with reward=-134.0\n",
      "iter=2900\tepsilon=0.102\n",
      "Current score(mean over 3) = -230.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 2998/5000 [04:20<02:45, 12.10it/s][2017-05-05 17:29:07,704] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:29:07,711] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:29:07,713] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:29:07,962] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 60%|██████    | 3000/5000 [04:21<04:02,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 169 timesteps with reward=-168.0\n",
      "Episode finished after 264 timesteps with reward=-263.0\n",
      "Episode finished after 276 timesteps with reward=-275.0\n",
      "iter=3000\tepsilon=0.097\n",
      "Current score(mean over 3) = -235.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 3098/5000 [04:29<02:37, 12.06it/s][2017-05-05 17:29:16,583] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:29:16,591] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:29:16,592] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:29:16,845] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 62%|██████▏   | 3100/5000 [04:30<03:52,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 190 timesteps with reward=-189.0\n",
      "Episode finished after 192 timesteps with reward=-191.0\n",
      "Episode finished after 314 timesteps with reward=-313.0\n",
      "iter=3100\tepsilon=0.093\n",
      "Current score(mean over 3) = -231.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 3199/5000 [04:39<02:42, 11.07it/s][2017-05-05 17:29:25,791] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:29:25,799] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:29:25,801] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "Episode finished after 500 timesteps with reward=-500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:29:26,292] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 64%|██████▍   | 3201/5000 [04:39<04:58,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 397 timesteps with reward=-396.0\n",
      "iter=3200\tepsilon=0.089\n",
      "Current score(mean over 3) = -465.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 3299/5000 [04:49<02:38, 10.70it/s][2017-05-05 17:29:36,043] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:29:36,051] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:29:36,053] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "Episode finished after 354 timesteps with reward=-353.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:29:36,521] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 66%|██████▌   | 3301/5000 [04:49<04:49,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 397 timesteps with reward=-396.0\n",
      "iter=3300\tepsilon=0.085\n",
      "Current score(mean over 3) = -416.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3399/5000 [04:59<02:33, 10.43it/s][2017-05-05 17:29:46,505] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:29:46,512] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:29:46,513] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:29:46,739] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 222 timesteps with reward=-221.0\n",
      "Episode finished after 297 timesteps with reward=-296.0\n",
      "Episode finished after 97 timesteps with reward=-96.0\n",
      "iter=3400\tepsilon=0.082\n",
      "Current score(mean over 3) = -204.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 3498/5000 [05:09<02:20, 10.67it/s][2017-05-05 17:29:56,334] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:29:56,344] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:29:56,346] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:29:56,591] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 70%|███████   | 3500/5000 [05:09<03:14,  7.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 232 timesteps with reward=-231.0\n",
      "Episode finished after 162 timesteps with reward=-161.0\n",
      "Episode finished after 96 timesteps with reward=-95.0\n",
      "iter=3500\tepsilon=0.079\n",
      "Current score(mean over 3) = -162.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 3598/5000 [05:19<02:10, 10.72it/s][2017-05-05 17:30:06,678] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:30:06,686] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:30:06,687] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 169 timesteps with reward=-168.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:30:07,035] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 72%|███████▏  | 3600/5000 [05:20<03:22,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 480 timesteps with reward=-479.0\n",
      "Episode finished after 183 timesteps with reward=-182.0\n",
      "iter=3600\tepsilon=0.076\n",
      "Current score(mean over 3) = -276.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 3698/5000 [05:30<02:20,  9.27it/s][2017-05-05 17:30:17,133] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:30:17,141] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:30:17,142] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 101 timesteps with reward=-100.0\n",
      "Episode finished after 500 timesteps with reward=-500.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:30:17,489] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 74%|███████▍  | 3702/5000 [05:31<02:56,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode finished after 249 timesteps with reward=-248.0\n",
      "iter=3700\tepsilon=0.073\n",
      "Current score(mean over 3) = -282.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 3799/5000 [05:40<01:53, 10.55it/s][2017-05-05 17:30:27,065] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:30:27,073] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:30:27,074] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:30:27,324] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 308 timesteps with reward=-307.0\n",
      "Episode finished after 160 timesteps with reward=-159.0\n",
      "Episode finished after 156 timesteps with reward=-155.0\n",
      "iter=3800\tepsilon=0.071\n",
      "Current score(mean over 3) = -207.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 3898/5000 [05:50<01:32, 11.92it/s][2017-05-05 17:30:36,817] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:30:36,825] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:30:36,826] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:30:37,014] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 78%|███████▊  | 3900/5000 [05:50<02:05,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 100 timesteps with reward=-99.0\n",
      "Episode finished after 326 timesteps with reward=-325.0\n",
      "Episode finished after 102 timesteps with reward=-101.0\n",
      "iter=3900\tepsilon=0.069\n",
      "Current score(mean over 3) = -175.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 3998/5000 [05:58<01:22, 12.12it/s][2017-05-05 17:30:45,424] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:30:45,431] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:30:45,433] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:30:45,716] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 80%|████████  | 4000/5000 [05:59<02:05,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 251 timesteps with reward=-250.0\n",
      "Episode finished after 321 timesteps with reward=-320.0\n",
      "Episode finished after 227 timesteps with reward=-226.0\n",
      "iter=4000\tepsilon=0.067\n",
      "Current score(mean over 3) = -265.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 4099/5000 [06:08<01:24, 10.60it/s][2017-05-05 17:30:54,950] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:30:54,958] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:30:54,959] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:30:55,169] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 170 timesteps with reward=-169.0\n",
      "Episode finished after 166 timesteps with reward=-165.0\n",
      "Episode finished after 146 timesteps with reward=-145.0\n",
      "iter=4100\tepsilon=0.066\n",
      "Current score(mean over 3) = -159.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 4199/5000 [06:17<01:16, 10.48it/s][2017-05-05 17:31:04,370] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:31:04,377] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:31:04,379] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:31:04,488] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 84%|████████▍ | 4201/5000 [06:17<01:28,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 109 timesteps with reward=-108.0\n",
      "Episode finished after 98 timesteps with reward=-97.0\n",
      "Episode finished after 77 timesteps with reward=-76.0\n",
      "iter=4200\tepsilon=0.064\n",
      "Current score(mean over 3) = -93.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 4298/5000 [06:27<01:08, 10.21it/s][2017-05-05 17:31:14,388] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:31:14,395] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:31:14,397] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:31:14,559] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 86%|████████▌ | 4300/5000 [06:27<01:24,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 87 timesteps with reward=-86.0\n",
      "Episode finished after 84 timesteps with reward=-83.0\n",
      "Episode finished after 110 timesteps with reward=-109.0\n",
      "iter=4300\tepsilon=0.063\n",
      "Current score(mean over 3) = -92.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 4398/5000 [06:37<00:59, 10.07it/s][2017-05-05 17:31:24,362] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:31:24,371] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:31:24,373] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:31:24,580] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 88%|████████▊ | 4400/5000 [06:37<01:24,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 84 timesteps with reward=-83.0\n",
      "Episode finished after 86 timesteps with reward=-85.0\n",
      "Episode finished after 86 timesteps with reward=-85.0\n",
      "iter=4400\tepsilon=0.062\n",
      "Current score(mean over 3) = -84.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 4498/5000 [06:47<00:55,  9.03it/s][2017-05-05 17:31:34,842] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:31:34,852] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:31:34,854] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:31:35,064] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 90%|█████████ | 4500/5000 [06:48<01:11,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 77 timesteps with reward=-76.0\n",
      "Episode finished after 147 timesteps with reward=-146.0\n",
      "Episode finished after 83 timesteps with reward=-82.0\n",
      "iter=4500\tepsilon=0.061\n",
      "Current score(mean over 3) = -101.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 4598/5000 [06:57<00:33, 12.11it/s][2017-05-05 17:31:44,218] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:31:44,225] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:31:44,226] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:31:44,366] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 92%|█████████▏| 4600/5000 [06:57<00:42,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 130 timesteps with reward=-129.0\n",
      "Episode finished after 114 timesteps with reward=-113.0\n",
      "Episode finished after 116 timesteps with reward=-115.0\n",
      "iter=4600\tepsilon=0.060\n",
      "Current score(mean over 3) = -119.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 4698/5000 [07:05<00:24, 12.25it/s][2017-05-05 17:31:52,637] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:31:52,645] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:31:52,646] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:31:52,944] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 94%|█████████▍| 4700/5000 [07:06<00:38,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 225 timesteps with reward=-224.0\n",
      "Episode finished after 240 timesteps with reward=-239.0\n",
      "Episode finished after 363 timesteps with reward=-362.0\n",
      "iter=4700\tepsilon=0.059\n",
      "Current score(mean over 3) = -275.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 4799/5000 [07:15<00:21,  9.50it/s][2017-05-05 17:32:02,392] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:32:02,401] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:32:02,402] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:32:02,624] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 96%|█████████▌| 4800/5000 [07:15<00:34,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 139 timesteps with reward=-138.0\n",
      "Episode finished after 131 timesteps with reward=-130.0\n",
      "Episode finished after 113 timesteps with reward=-112.0\n",
      "iter=4800\tepsilon=0.058\n",
      "Current score(mean over 3) = -126.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 4899/5000 [07:25<00:10,  9.79it/s][2017-05-05 17:32:12,439] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:32:12,447] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:32:12,448] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-05 17:32:12,584] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      " 98%|█████████▊| 4900/5000 [07:25<00:14,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 93 timesteps with reward=-92.0\n",
      "Episode finished after 92 timesteps with reward=-91.0\n",
      "Episode finished after 100 timesteps with reward=-99.0\n",
      "iter=4900\tepsilon=0.057\n",
      "Current score(mean over 3) = -94.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 4998/5000 [07:36<00:00, 10.97it/s][2017-05-05 17:32:22,854] Making new env: Acrobot-v1\n",
      "[2017-05-05 17:32:22,862] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-05-05 17:32:22,863] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 457 timesteps with reward=-456.0\n",
      "Episode finished after 273 timesteps with reward=-272.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-05 17:32:23,301] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Documents/Homeworks/rl/reinforcement_learning/week4/records')\n",
      "100%|██████████| 5000/5000 [07:36<00:00,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "iter=5000\tepsilon=0.056\n",
      "Current score(mean over 3) = -409.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "target_score = 100\n",
    "\n",
    "for i in trange(5000):    \n",
    "    \n",
    "    #play\n",
    "    for _ in range(5):\n",
    "        pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    #train\n",
    "    train_step()\n",
    "    \n",
    "    #update epsilon\n",
    "    epsilon = 0.05 + 0.95*np.exp(-epoch_counter/1000.)\n",
    "    action_layer.epsilon.set_value(np.float32(epsilon))\n",
    "    \n",
    "    #play a few games for evaluation\n",
    "    if epoch_counter%100==0:\n",
    "        rewards[epoch_counter] = np.mean(pool.evaluate(n_games=3,record_video=False))\n",
    "        print(\"iter=%i\\tepsilon=%.3f\"%(epoch_counter,action_layer.epsilon.get_value(),))\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(3,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "        if rewards[epoch_counter] >= target_score:\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import ewma\n",
    "iters,session_rewards=zip(*sorted(rewards.items(),key=lambda (k,v):k))\n",
    "plt.plot(iters,ewma(np.array(session_rewards),span=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 01:44:13,614] Making new env: Acrobot-v1\n",
      "[2017-02-28 01:44:13,624] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-02-28 01:44:13,626] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-02-28 01:44:13,629] Starting new video recorder writing to /Users/oleg/Dropbox/Homeworks/rl/rl_w4/week4/records/openaigym.video.23.63032.video000000.mp4\n",
      "[2017-02-28 01:44:16,865] Starting new video recorder writing to /Users/oleg/Dropbox/Homeworks/rl/rl_w4/week4/records/openaigym.video.23.63032.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 255 timesteps with reward=-254.0\n",
      "Episode finished after 218 timesteps with reward=-217.0\n",
      "Episode finished after 249 timesteps with reward=-248.0\n",
      "Episode finished after 270 timesteps with reward=-269.0\n",
      "Episode finished after 180 timesteps with reward=-179.0\n",
      "Episode finished after 318 timesteps with reward=-317.0\n",
      "Episode finished after 170 timesteps with reward=-169.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 01:44:20,456] Starting new video recorder writing to /Users/oleg/Dropbox/Homeworks/rl/rl_w4/week4/records/openaigym.video.23.63032.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps with reward=-500.0\n",
      "Episode finished after 240 timesteps with reward=-239.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-28 01:44:23,842] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/oleg/Dropbox/Homeworks/rl/rl_w4/week4/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 364 timesteps with reward=-363.0\n",
      "('average reward:', [-254.0, -217.0, -248.0, -269.0, -179.0, -317.0, -169.0, -500.0, -239.0, -363.0])\n"
     ]
    }
   ],
   "source": [
    "final_reward = pool.evaluate(n_games=10,save_path=\"./records\",record_video=True)\n",
    "\n",
    "print(\"average reward:\",final_reward)\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "for video_name in video_names:\n",
    "    HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"{}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\".format(\"./records/\"+video_name)) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Homework part I (5+ pts)\n",
    "\n",
    "Train a neural network for [`LunarLander-v2`](https://gym.openai.com/envs/LunarLander-v2).\n",
    "* Getting average reward of at least +0 gets you 5 points\n",
    "* Higher reward = more points\n",
    "\n",
    "\n",
    "## Bonus I\n",
    "* Try getting the same [or better] results on Acrobot __(+2 pts)__ or __LunarLander (+3 pts)__ using on-policy methods\n",
    "* You can get n-step q-learning by messing with ```n_steps``` param in the q-learning code above\n",
    "* Note that using large experience replay buffer will slow down on-policy algorithms to almost zero, so it's probably a good idea to use small experience replay buffer with several parallel agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
